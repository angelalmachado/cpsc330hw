{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning\n",
    "\n",
    "## Homework 8: Time Series and Survival analysis (Lectures 19 and 20) \n",
    "\n",
    "**Due date: see the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:4}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 4. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Exercise 1: time series prediction\n",
    "\n",
    "In this exercise we'll be looking at a [dataset of avocado prices](https://www.kaggle.com/neuromusic/avocado-prices). You should start by downloading the dataset. We will be forcasting average avocado price for the next week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1.33</td>\n",
       "      <td>64236.62</td>\n",
       "      <td>1036.74</td>\n",
       "      <td>54454.85</td>\n",
       "      <td>48.16</td>\n",
       "      <td>8696.87</td>\n",
       "      <td>8603.62</td>\n",
       "      <td>93.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>54876.98</td>\n",
       "      <td>674.28</td>\n",
       "      <td>44638.81</td>\n",
       "      <td>58.33</td>\n",
       "      <td>9505.56</td>\n",
       "      <td>9408.07</td>\n",
       "      <td>97.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-13</td>\n",
       "      <td>0.93</td>\n",
       "      <td>118220.22</td>\n",
       "      <td>794.70</td>\n",
       "      <td>109149.67</td>\n",
       "      <td>130.50</td>\n",
       "      <td>8145.35</td>\n",
       "      <td>8042.21</td>\n",
       "      <td>103.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-06</td>\n",
       "      <td>1.08</td>\n",
       "      <td>78992.15</td>\n",
       "      <td>1132.00</td>\n",
       "      <td>71976.41</td>\n",
       "      <td>72.58</td>\n",
       "      <td>5811.16</td>\n",
       "      <td>5677.40</td>\n",
       "      <td>133.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-11-29</td>\n",
       "      <td>1.28</td>\n",
       "      <td>51039.60</td>\n",
       "      <td>941.48</td>\n",
       "      <td>43838.39</td>\n",
       "      <td>75.78</td>\n",
       "      <td>6183.95</td>\n",
       "      <td>5986.26</td>\n",
       "      <td>197.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume     4046       4225    4770  \\\n",
       "0 2015-12-27          1.33      64236.62  1036.74   54454.85   48.16   \n",
       "1 2015-12-20          1.35      54876.98   674.28   44638.81   58.33   \n",
       "2 2015-12-13          0.93     118220.22   794.70  109149.67  130.50   \n",
       "3 2015-12-06          1.08      78992.15  1132.00   71976.41   72.58   \n",
       "4 2015-11-29          1.28      51039.60   941.48   43838.39   75.78   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  region  \n",
       "0     8696.87     8603.62       93.25          0.0  conventional  2015  Albany  \n",
       "1     9505.56     9408.07       97.49          0.0  conventional  2015  Albany  \n",
       "2     8145.35     8042.21      103.14          0.0  conventional  2015  Albany  \n",
       "3     5811.16     5677.40      133.76          0.0  conventional  2015  Albany  \n",
       "4     6183.95     5986.26      197.69          0.0  conventional  2015  Albany  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/avocado.csv\", parse_dates=[\"Date\"], index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18249, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-01-04 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-03-25 00:00:00')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data ranges from the start of 2015 to March 2018 (~2 years ago), for a total of 3.25 years or so. Let's split the data so that we have a 6 months of test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '20170925'\n",
    "df_train = df[df[\"Date\"] <= split_date]\n",
    "df_test  = df[df[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_test) == len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many time series? \n",
    "rubric={points:4}\n",
    "\n",
    "In the [Rain in Australia](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package) dataset from lecture, we had different measurements for each Location. \n",
    "\n",
    "We want you to consider this for the avocado prices dataset. For which categorical feature(s), if any, do we have separate measurements? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we have separate measurements for both categorical features in this data set: \"region\" and \"type\". When we sort the dataframe by date, we see that, for the same date, there are measurements for different regions. When we further sort the dataframe by region (as well as date), we see that, for the same date AND region, there are separate measurements for the different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18249 entries, 0 to 11\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   Date          18249 non-null  datetime64[ns]\n",
      " 1   AveragePrice  18249 non-null  float64       \n",
      " 2   Total Volume  18249 non-null  float64       \n",
      " 3   4046          18249 non-null  float64       \n",
      " 4   4225          18249 non-null  float64       \n",
      " 5   4770          18249 non-null  float64       \n",
      " 6   Total Bags    18249 non-null  float64       \n",
      " 7   Small Bags    18249 non-null  float64       \n",
      " 8   Large Bags    18249 non-null  float64       \n",
      " 9   XLarge Bags   18249 non-null  float64       \n",
      " 10  type          18249 non-null  object        \n",
      " 11  year          18249 non-null  int64         \n",
      " 12  region        18249 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(2)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like type and region are the only categorical variables in this dataset. Looking at their value counts, we see two potential values for the type feature and many different values (~50) for the region feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "conventional    9126\n",
       "organic         9123\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region\n",
       "Albany                 338\n",
       "Sacramento             338\n",
       "Northeast              338\n",
       "NorthernNewEngland     338\n",
       "Orlando                338\n",
       "Philadelphia           338\n",
       "PhoenixTucson          338\n",
       "Pittsburgh             338\n",
       "Plains                 338\n",
       "Portland               338\n",
       "RaleighGreensboro      338\n",
       "RichmondNorfolk        338\n",
       "Roanoke                338\n",
       "SanDiego               338\n",
       "Atlanta                338\n",
       "SanFrancisco           338\n",
       "Seattle                338\n",
       "SouthCarolina          338\n",
       "SouthCentral           338\n",
       "Southeast              338\n",
       "Spokane                338\n",
       "StLouis                338\n",
       "Syracuse               338\n",
       "Tampa                  338\n",
       "TotalUS                338\n",
       "West                   338\n",
       "NewYork                338\n",
       "NewOrleansMobile       338\n",
       "Nashville              338\n",
       "Midsouth               338\n",
       "BaltimoreWashington    338\n",
       "Boise                  338\n",
       "Boston                 338\n",
       "BuffaloRochester       338\n",
       "California             338\n",
       "Charlotte              338\n",
       "Chicago                338\n",
       "CincinnatiDayton       338\n",
       "Columbus               338\n",
       "DallasFtWorth          338\n",
       "Denver                 338\n",
       "Detroit                338\n",
       "GrandRapids            338\n",
       "GreatLakes             338\n",
       "HarrisburgScranton     338\n",
       "HartfordSpringfield    338\n",
       "Houston                338\n",
       "Indianapolis           338\n",
       "Jacksonville           338\n",
       "LasVegas               338\n",
       "LosAngeles             338\n",
       "Louisville             338\n",
       "MiamiFtLauderdale      338\n",
       "WestTexNewMexico       335\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"region\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>2.02</td>\n",
       "      <td>13379.77</td>\n",
       "      <td>86.84</td>\n",
       "      <td>5923.16</td>\n",
       "      <td>98.37</td>\n",
       "      <td>7271.40</td>\n",
       "      <td>6881.80</td>\n",
       "      <td>389.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>RaleighGreensboro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1774776.77</td>\n",
       "      <td>63905.98</td>\n",
       "      <td>908653.71</td>\n",
       "      <td>843.45</td>\n",
       "      <td>801373.63</td>\n",
       "      <td>774634.09</td>\n",
       "      <td>23833.93</td>\n",
       "      <td>2905.61</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>NewYork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.45</td>\n",
       "      <td>121917.39</td>\n",
       "      <td>1929.39</td>\n",
       "      <td>18391.86</td>\n",
       "      <td>110.05</td>\n",
       "      <td>101486.09</td>\n",
       "      <td>85313.41</td>\n",
       "      <td>16172.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>Southeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.19</td>\n",
       "      <td>450658.34</td>\n",
       "      <td>205952.19</td>\n",
       "      <td>73267.19</td>\n",
       "      <td>4313.11</td>\n",
       "      <td>167125.85</td>\n",
       "      <td>132488.40</td>\n",
       "      <td>31527.74</td>\n",
       "      <td>3109.71</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>SouthCarolina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.40</td>\n",
       "      <td>524265.69</td>\n",
       "      <td>103573.88</td>\n",
       "      <td>149867.07</td>\n",
       "      <td>998.53</td>\n",
       "      <td>269826.21</td>\n",
       "      <td>155866.80</td>\n",
       "      <td>113666.70</td>\n",
       "      <td>292.71</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume       4046       4225     4770  \\\n",
       "0 2018-03-25          2.02      13379.77      86.84    5923.16    98.37   \n",
       "0 2018-03-25          1.34    1774776.77   63905.98  908653.71   843.45   \n",
       "0 2018-03-25          1.45     121917.39    1929.39   18391.86   110.05   \n",
       "0 2018-03-25          1.19     450658.34  205952.19   73267.19  4313.11   \n",
       "0 2018-03-25          1.40     524265.69  103573.88  149867.07   998.53   \n",
       "\n",
       "   Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0     7271.40     6881.80      389.60         0.00       organic  2018   \n",
       "0   801373.63   774634.09    23833.93      2905.61  conventional  2018   \n",
       "0   101486.09    85313.41    16172.68         0.00       organic  2018   \n",
       "0   167125.85   132488.40    31527.74      3109.71  conventional  2018   \n",
       "0   269826.21   155866.80   113666.70       292.71  conventional  2018   \n",
       "\n",
       "              region  \n",
       "0  RaleighGreensboro  \n",
       "0            NewYork  \n",
       "0          Southeast  \n",
       "0      SouthCarolina  \n",
       "0            Seattle  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date = df.sort_values(by=\"Date\", ascending=False)\n",
    "df_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>0.84</td>\n",
       "      <td>965185.06</td>\n",
       "      <td>438526.12</td>\n",
       "      <td>199585.90</td>\n",
       "      <td>11017.42</td>\n",
       "      <td>316055.62</td>\n",
       "      <td>153009.89</td>\n",
       "      <td>160999.10</td>\n",
       "      <td>2046.63</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7667064.46</td>\n",
       "      <td>2567279.74</td>\n",
       "      <td>1912986.38</td>\n",
       "      <td>118289.91</td>\n",
       "      <td>3068508.43</td>\n",
       "      <td>1309580.19</td>\n",
       "      <td>1745630.06</td>\n",
       "      <td>13298.18</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.60</td>\n",
       "      <td>271723.08</td>\n",
       "      <td>26996.28</td>\n",
       "      <td>77861.39</td>\n",
       "      <td>117.56</td>\n",
       "      <td>166747.85</td>\n",
       "      <td>87108.00</td>\n",
       "      <td>79495.39</td>\n",
       "      <td>144.46</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.03</td>\n",
       "      <td>43409835.75</td>\n",
       "      <td>14130799.10</td>\n",
       "      <td>12125711.42</td>\n",
       "      <td>758801.12</td>\n",
       "      <td>16394524.11</td>\n",
       "      <td>12540327.19</td>\n",
       "      <td>3544729.39</td>\n",
       "      <td>309467.53</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2018</td>\n",
       "      <td>TotalUS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  AveragePrice  Total Volume         4046         4225       4770  \\\n",
       "0 2018-03-25          0.84     965185.06    438526.12    199585.90   11017.42   \n",
       "0 2018-03-25          1.62      15303.40      2325.30      2171.66       0.00   \n",
       "0 2018-03-25          0.93    7667064.46   2567279.74   1912986.38  118289.91   \n",
       "0 2018-03-25          1.60     271723.08     26996.28     77861.39     117.56   \n",
       "0 2018-03-25          1.03   43409835.75  14130799.10  12125711.42  758801.12   \n",
       "\n",
       "    Total Bags   Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0    316055.62    153009.89   160999.10      2046.63  conventional  2018   \n",
       "0     10806.44     10569.80      236.64         0.00       organic  2018   \n",
       "0   3068508.43   1309580.19  1745630.06     13298.18  conventional  2018   \n",
       "0    166747.85     87108.00    79495.39       144.46       organic  2018   \n",
       "0  16394524.11  12540327.19  3544729.39    309467.53  conventional  2018   \n",
       "\n",
       "             region  \n",
       "0  WestTexNewMexico  \n",
       "0  WestTexNewMexico  \n",
       "0              West  \n",
       "0              West  \n",
       "0           TotalUS  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date = df.sort_values(by=[\"Date\", \"region\"], ascending=False)\n",
    "df_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Equally spaced measurements? \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, the measurements were generally equally spaced but with some exceptions. How about with this dataset? Justify your answer by referencing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the measurements are generally spaced equally. They are separated by 7 days. The only exception is for the `region` WestTexNewMexico of `type` organic where the spacings are mostly 7 days but there is also spacing of 14 and 21 days. In terms of missing values, looking at the outputs from `size()`, the measurement differences, and `isnat()`, we can see that there is 1 missing measurement for each type at every region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region               type        \n",
      "Albany               conventional    169\n",
      "                     organic         169\n",
      "Atlanta              conventional    169\n",
      "                     organic         169\n",
      "BaltimoreWashington  conventional    169\n",
      "                                    ... \n",
      "TotalUS              organic         169\n",
      "West                 conventional    169\n",
      "                     organic         169\n",
      "WestTexNewMexico     conventional    169\n",
      "                     organic         166\n",
      "Length: 108, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "region_type = df.groupby(['region', 'type'])\n",
    "print(region_type.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Albany, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Albany, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Atlanta, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Atlanta, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(BaltimoreWashington, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(BaltimoreWashington, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Boise, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Boise, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Boston, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Boston, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(BuffaloRochester, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(BuffaloRochester, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(California, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(California, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Charlotte, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Charlotte, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Chicago, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Chicago, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(CincinnatiDayton, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(CincinnatiDayton, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Columbus, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Columbus, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(DallasFtWorth, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(DallasFtWorth, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Denver, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Denver, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Detroit, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Detroit, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(GrandRapids, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(GrandRapids, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(GreatLakes, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(GreatLakes, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(HarrisburgScranton, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(HarrisburgScranton, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(HartfordSpringfield, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(HartfordSpringfield, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Houston, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Houston, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Indianapolis, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Indianapolis, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Jacksonville, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Jacksonville, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(LasVegas, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(LasVegas, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(LosAngeles, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(LosAngeles, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Louisville, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Louisville, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(MiamiFtLauderdale, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(MiamiFtLauderdale, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Midsouth, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Midsouth, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Nashville, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Nashville, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(NewOrleansMobile, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(NewOrleansMobile, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(NewYork, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(NewYork, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Northeast, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Northeast, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(NorthernNewEngland, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(NorthernNewEngland, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Orlando, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Orlando, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Philadelphia, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Philadelphia, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(PhoenixTucson, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(PhoenixTucson, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Pittsburgh, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Pittsburgh, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Plains, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Plains, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Portland, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Portland, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(RaleighGreensboro, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(RaleighGreensboro, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(RichmondNorfolk, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(RichmondNorfolk, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Roanoke, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Roanoke, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Sacramento, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Sacramento, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SanDiego, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SanDiego, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SanFrancisco, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SanFrancisco, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Seattle, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Seattle, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SouthCarolina, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SouthCarolina, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SouthCentral, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(SouthCentral, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Southeast, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Southeast, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Spokane, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Spokane, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(StLouis, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(StLouis, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Syracuse, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Syracuse, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Tampa, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(Tampa, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(TotalUS, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(TotalUS, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(West, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(West, organic): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(WestTexNewMexico, conventional): Date\n",
      "7 days    168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(WestTexNewMexico, organic): Date\n",
      "7 days     163\n",
      "14 days      1\n",
      "21 days      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adapted from Lecture 19\n",
    "for (region, type), group in region_type:\n",
    "    diff = group[\"Date\"].sort_values().diff()\n",
    "    print(\"(%s, %s): %s\" % (region, type, diff.value_counts()))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in (Albany, conventional): 1\n",
      "Number of missing values in (Albany, organic): 1\n",
      "Number of missing values in (Atlanta, conventional): 1\n",
      "Number of missing values in (Atlanta, organic): 1\n",
      "Number of missing values in (BaltimoreWashington, conventional): 1\n",
      "Number of missing values in (BaltimoreWashington, organic): 1\n",
      "Number of missing values in (Boise, conventional): 1\n",
      "Number of missing values in (Boise, organic): 1\n",
      "Number of missing values in (Boston, conventional): 1\n",
      "Number of missing values in (Boston, organic): 1\n",
      "Number of missing values in (BuffaloRochester, conventional): 1\n",
      "Number of missing values in (BuffaloRochester, organic): 1\n",
      "Number of missing values in (California, conventional): 1\n",
      "Number of missing values in (California, organic): 1\n",
      "Number of missing values in (Charlotte, conventional): 1\n",
      "Number of missing values in (Charlotte, organic): 1\n",
      "Number of missing values in (Chicago, conventional): 1\n",
      "Number of missing values in (Chicago, organic): 1\n",
      "Number of missing values in (CincinnatiDayton, conventional): 1\n",
      "Number of missing values in (CincinnatiDayton, organic): 1\n",
      "Number of missing values in (Columbus, conventional): 1\n",
      "Number of missing values in (Columbus, organic): 1\n",
      "Number of missing values in (DallasFtWorth, conventional): 1\n",
      "Number of missing values in (DallasFtWorth, organic): 1\n",
      "Number of missing values in (Denver, conventional): 1\n",
      "Number of missing values in (Denver, organic): 1\n",
      "Number of missing values in (Detroit, conventional): 1\n",
      "Number of missing values in (Detroit, organic): 1\n",
      "Number of missing values in (GrandRapids, conventional): 1\n",
      "Number of missing values in (GrandRapids, organic): 1\n",
      "Number of missing values in (GreatLakes, conventional): 1\n",
      "Number of missing values in (GreatLakes, organic): 1\n",
      "Number of missing values in (HarrisburgScranton, conventional): 1\n",
      "Number of missing values in (HarrisburgScranton, organic): 1\n",
      "Number of missing values in (HartfordSpringfield, conventional): 1\n",
      "Number of missing values in (HartfordSpringfield, organic): 1\n",
      "Number of missing values in (Houston, conventional): 1\n",
      "Number of missing values in (Houston, organic): 1\n",
      "Number of missing values in (Indianapolis, conventional): 1\n",
      "Number of missing values in (Indianapolis, organic): 1\n",
      "Number of missing values in (Jacksonville, conventional): 1\n",
      "Number of missing values in (Jacksonville, organic): 1\n",
      "Number of missing values in (LasVegas, conventional): 1\n",
      "Number of missing values in (LasVegas, organic): 1\n",
      "Number of missing values in (LosAngeles, conventional): 1\n",
      "Number of missing values in (LosAngeles, organic): 1\n",
      "Number of missing values in (Louisville, conventional): 1\n",
      "Number of missing values in (Louisville, organic): 1\n",
      "Number of missing values in (MiamiFtLauderdale, conventional): 1\n",
      "Number of missing values in (MiamiFtLauderdale, organic): 1\n",
      "Number of missing values in (Midsouth, conventional): 1\n",
      "Number of missing values in (Midsouth, organic): 1\n",
      "Number of missing values in (Nashville, conventional): 1\n",
      "Number of missing values in (Nashville, organic): 1\n",
      "Number of missing values in (NewOrleansMobile, conventional): 1\n",
      "Number of missing values in (NewOrleansMobile, organic): 1\n",
      "Number of missing values in (NewYork, conventional): 1\n",
      "Number of missing values in (NewYork, organic): 1\n",
      "Number of missing values in (Northeast, conventional): 1\n",
      "Number of missing values in (Northeast, organic): 1\n",
      "Number of missing values in (NorthernNewEngland, conventional): 1\n",
      "Number of missing values in (NorthernNewEngland, organic): 1\n",
      "Number of missing values in (Orlando, conventional): 1\n",
      "Number of missing values in (Orlando, organic): 1\n",
      "Number of missing values in (Philadelphia, conventional): 1\n",
      "Number of missing values in (Philadelphia, organic): 1\n",
      "Number of missing values in (PhoenixTucson, conventional): 1\n",
      "Number of missing values in (PhoenixTucson, organic): 1\n",
      "Number of missing values in (Pittsburgh, conventional): 1\n",
      "Number of missing values in (Pittsburgh, organic): 1\n",
      "Number of missing values in (Plains, conventional): 1\n",
      "Number of missing values in (Plains, organic): 1\n",
      "Number of missing values in (Portland, conventional): 1\n",
      "Number of missing values in (Portland, organic): 1\n",
      "Number of missing values in (RaleighGreensboro, conventional): 1\n",
      "Number of missing values in (RaleighGreensboro, organic): 1\n",
      "Number of missing values in (RichmondNorfolk, conventional): 1\n",
      "Number of missing values in (RichmondNorfolk, organic): 1\n",
      "Number of missing values in (Roanoke, conventional): 1\n",
      "Number of missing values in (Roanoke, organic): 1\n",
      "Number of missing values in (Sacramento, conventional): 1\n",
      "Number of missing values in (Sacramento, organic): 1\n",
      "Number of missing values in (SanDiego, conventional): 1\n",
      "Number of missing values in (SanDiego, organic): 1\n",
      "Number of missing values in (SanFrancisco, conventional): 1\n",
      "Number of missing values in (SanFrancisco, organic): 1\n",
      "Number of missing values in (Seattle, conventional): 1\n",
      "Number of missing values in (Seattle, organic): 1\n",
      "Number of missing values in (SouthCarolina, conventional): 1\n",
      "Number of missing values in (SouthCarolina, organic): 1\n",
      "Number of missing values in (SouthCentral, conventional): 1\n",
      "Number of missing values in (SouthCentral, organic): 1\n",
      "Number of missing values in (Southeast, conventional): 1\n",
      "Number of missing values in (Southeast, organic): 1\n",
      "Number of missing values in (Spokane, conventional): 1\n",
      "Number of missing values in (Spokane, organic): 1\n",
      "Number of missing values in (StLouis, conventional): 1\n",
      "Number of missing values in (StLouis, organic): 1\n",
      "Number of missing values in (Syracuse, conventional): 1\n",
      "Number of missing values in (Syracuse, organic): 1\n",
      "Number of missing values in (Tampa, conventional): 1\n",
      "Number of missing values in (Tampa, organic): 1\n",
      "Number of missing values in (TotalUS, conventional): 1\n",
      "Number of missing values in (TotalUS, organic): 1\n",
      "Number of missing values in (West, conventional): 1\n",
      "Number of missing values in (West, organic): 1\n",
      "Number of missing values in (WestTexNewMexico, conventional): 1\n",
      "Number of missing values in (WestTexNewMexico, organic): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p5/hwwszq5n2kv_18jljl_3nq1h0000gn/T/ipykernel_41171/1436522757.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  missing = np.isnat(diff).value_counts()[1]\n"
     ]
    }
   ],
   "source": [
    "# Adapted from https://numpy.org/doc/stable/reference/generated/numpy.isnat.html\n",
    "# checking for missing values\n",
    "for (region, type), group in region_type:\n",
    "    diff = group[\"Date\"].sort_values().diff()\n",
    "    missing = np.isnat(diff).value_counts()[1]\n",
    "    print(\"Number of missing values in (%s, %s): %s\" % (region, type, missing)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Interpreting regions \n",
    "rubric={points:4}\n",
    "\n",
    "In the Rain in Australia dataset, each location was a different place in Australia. For this dataset, look at the names of the regions. Do you think the regions are also all distinct, or are there overlapping regions? Justify your answer by referencing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the unique values in `region` column, there are overlapping regions. For this feature, names of cities, states, regions, and the country itself are included and these overlap with each other. For example, both 'LosAngeles' and 'California' are included here when 'LosAngeles' is a city in 'California'. Similarly, 'SouthCentral' is a region used to collectively identify a few states in the South of the US, and 'Houston' is one of the cities within in this region, but in this dataset, both 'Houston' and 'SouthCentral' are included as individual values. 'TotalUS', from its name at least, refers to the entire country however it is included as an individual value along with other US cities, states, and regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albany', 'Atlanta', 'BaltimoreWashington', 'Boise', 'Boston',\n",
       "       'BuffaloRochester', 'California', 'Charlotte', 'Chicago',\n",
       "       'CincinnatiDayton', 'Columbus', 'DallasFtWorth', 'Denver',\n",
       "       'Detroit', 'GrandRapids', 'GreatLakes', 'HarrisburgScranton',\n",
       "       'HartfordSpringfield', 'Houston', 'Indianapolis', 'Jacksonville',\n",
       "       'LasVegas', 'LosAngeles', 'Louisville', 'MiamiFtLauderdale',\n",
       "       'Midsouth', 'Nashville', 'NewOrleansMobile', 'NewYork',\n",
       "       'Northeast', 'NorthernNewEngland', 'Orlando', 'Philadelphia',\n",
       "       'PhoenixTucson', 'Pittsburgh', 'Plains', 'Portland',\n",
       "       'RaleighGreensboro', 'RichmondNorfolk', 'Roanoke', 'Sacramento',\n",
       "       'SanDiego', 'SanFrancisco', 'Seattle', 'SouthCarolina',\n",
       "       'SouthCentral', 'Southeast', 'Spokane', 'StLouis', 'Syracuse',\n",
       "       'Tampa', 'TotalUS', 'West', 'WestTexNewMexico'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['region'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the entire dataset despite any location-based weirdness uncovered in the previous part.\n",
    "\n",
    "We will be trying to forecast the avocado price. The function below is adapted from [Lecture 19](https://github.com/UBC-CS/cpsc330-2023W1/tree/main/lectures), with some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(df, orig_feature, lag, groupby, new_feature_name=None, clip=False):\n",
    "    \"\"\"\n",
    "    Creates a new feature that's a lagged version of an existing one.\n",
    "    \n",
    "    NOTE: assumes df is already sorted by the time columns and has unique indices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.core.frame.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        The column name of the feature we're copying\n",
    "    lag : int\n",
    "        The lag; negative lag means values from the past, positive lag means values from the future\n",
    "    groupby : list\n",
    "        Column(s) to group by in case df contains multiple time series\n",
    "    new_feature_name : str\n",
    "        Override the default name of the newly created column\n",
    "    clip : bool\n",
    "        If True, remove rows with a NaN values for the new feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.core.frame.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = \"%s_lag%d\" % (orig_feature, -lag)\n",
    "        else:\n",
    "            new_feature_name = \"%s_ahead%d\" % (orig_feature, lag)\n",
    "    \n",
    "    new_df = df.assign(**{new_feature_name : np.nan})\n",
    "    for name, group in new_df.groupby(groupby):        \n",
    "        if lag < 0: # take values from the past\n",
    "            new_df.loc[group.index[-lag:],new_feature_name] = group.iloc[:lag][orig_feature].values\n",
    "        else:       # take values from the future\n",
    "            new_df.loc[group.index[:-lag], new_feature_name] = group.iloc[lag:][orig_feature].values\n",
    "            \n",
    "    if clip:\n",
    "        new_df = new_df.dropna(subset=[new_feature_name])\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first sort our dataframe properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18248</th>\n",
       "      <td>2018-03-25</td>\n",
       "      <td>1.62</td>\n",
       "      <td>15303.40</td>\n",
       "      <td>2325.30</td>\n",
       "      <td>2171.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10806.44</td>\n",
       "      <td>10569.80</td>\n",
       "      <td>236.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18249 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "18248 2018-03-25          1.62      15303.40  2325.30   2171.66    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "18248    10806.44    10569.80      236.64          0.0       organic  2018   \n",
       "\n",
       "                 region  \n",
       "0                Albany  \n",
       "1                Albany  \n",
       "2                Albany  \n",
       "3                Albany  \n",
       "4                Albany  \n",
       "...                 ...  \n",
       "18244  WestTexNewMexico  \n",
       "18245  WestTexNewMexico  \n",
       "18246  WestTexNewMexico  \n",
       "18247  WestTexNewMexico  \n",
       "18248  WestTexNewMexico  \n",
       "\n",
       "[18249 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sort = df.sort_values(by=[\"region\", \"type\", \"Date\"]).reset_index(drop=True)\n",
    "df_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call `create_lag_feature`. This creates a new column in the dataset `AveragePriceNextWeek`, which is the following week's `AveragePrice`. We have set `clip=True` which means it will remove rows where the target would be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>Total Volume</th>\n",
       "      <th>4046</th>\n",
       "      <th>4225</th>\n",
       "      <th>4770</th>\n",
       "      <th>Total Bags</th>\n",
       "      <th>Small Bags</th>\n",
       "      <th>Large Bags</th>\n",
       "      <th>XLarge Bags</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>region</th>\n",
       "      <th>AveragePriceNextWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-11</td>\n",
       "      <td>1.24</td>\n",
       "      <td>41195.08</td>\n",
       "      <td>1002.85</td>\n",
       "      <td>31640.34</td>\n",
       "      <td>127.12</td>\n",
       "      <td>8424.77</td>\n",
       "      <td>8036.04</td>\n",
       "      <td>388.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>44511.28</td>\n",
       "      <td>914.14</td>\n",
       "      <td>31540.32</td>\n",
       "      <td>135.77</td>\n",
       "      <td>11921.05</td>\n",
       "      <td>11651.09</td>\n",
       "      <td>269.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-25</td>\n",
       "      <td>1.06</td>\n",
       "      <td>45147.50</td>\n",
       "      <td>941.38</td>\n",
       "      <td>33196.16</td>\n",
       "      <td>164.14</td>\n",
       "      <td>10845.82</td>\n",
       "      <td>10103.35</td>\n",
       "      <td>742.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>70873.60</td>\n",
       "      <td>1353.90</td>\n",
       "      <td>60017.20</td>\n",
       "      <td>179.32</td>\n",
       "      <td>9323.18</td>\n",
       "      <td>9170.82</td>\n",
       "      <td>152.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>2015</td>\n",
       "      <td>Albany</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18243</th>\n",
       "      <td>2018-02-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>17597.12</td>\n",
       "      <td>1892.05</td>\n",
       "      <td>1928.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13776.71</td>\n",
       "      <td>13553.53</td>\n",
       "      <td>223.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18244</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>18421.24</td>\n",
       "      <td>1974.26</td>\n",
       "      <td>2482.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13964.33</td>\n",
       "      <td>13698.27</td>\n",
       "      <td>266.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18245</th>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>1.54</td>\n",
       "      <td>17393.30</td>\n",
       "      <td>1832.24</td>\n",
       "      <td>1905.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13655.49</td>\n",
       "      <td>13401.93</td>\n",
       "      <td>253.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18246</th>\n",
       "      <td>2018-03-11</td>\n",
       "      <td>1.56</td>\n",
       "      <td>22128.42</td>\n",
       "      <td>2162.67</td>\n",
       "      <td>3194.25</td>\n",
       "      <td>8.93</td>\n",
       "      <td>16762.57</td>\n",
       "      <td>16510.32</td>\n",
       "      <td>252.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18247</th>\n",
       "      <td>2018-03-18</td>\n",
       "      <td>1.56</td>\n",
       "      <td>15896.38</td>\n",
       "      <td>2055.35</td>\n",
       "      <td>1499.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12341.48</td>\n",
       "      <td>12114.81</td>\n",
       "      <td>226.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>2018</td>\n",
       "      <td>WestTexNewMexico</td>\n",
       "      <td>1.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18141 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  AveragePrice  Total Volume     4046      4225    4770  \\\n",
       "0     2015-01-04          1.22      40873.28  2819.50  28287.42   49.90   \n",
       "1     2015-01-11          1.24      41195.08  1002.85  31640.34  127.12   \n",
       "2     2015-01-18          1.17      44511.28   914.14  31540.32  135.77   \n",
       "3     2015-01-25          1.06      45147.50   941.38  33196.16  164.14   \n",
       "4     2015-02-01          0.99      70873.60  1353.90  60017.20  179.32   \n",
       "...          ...           ...           ...      ...       ...     ...   \n",
       "18243 2018-02-18          1.56      17597.12  1892.05   1928.36    0.00   \n",
       "18244 2018-02-25          1.57      18421.24  1974.26   2482.65    0.00   \n",
       "18245 2018-03-04          1.54      17393.30  1832.24   1905.57    0.00   \n",
       "18246 2018-03-11          1.56      22128.42  2162.67   3194.25    8.93   \n",
       "18247 2018-03-18          1.56      15896.38  2055.35   1499.55    0.00   \n",
       "\n",
       "       Total Bags  Small Bags  Large Bags  XLarge Bags          type  year  \\\n",
       "0         9716.46     9186.93      529.53          0.0  conventional  2015   \n",
       "1         8424.77     8036.04      388.73          0.0  conventional  2015   \n",
       "2        11921.05    11651.09      269.96          0.0  conventional  2015   \n",
       "3        10845.82    10103.35      742.47          0.0  conventional  2015   \n",
       "4         9323.18     9170.82      152.36          0.0  conventional  2015   \n",
       "...           ...         ...         ...          ...           ...   ...   \n",
       "18243    13776.71    13553.53      223.18          0.0       organic  2018   \n",
       "18244    13964.33    13698.27      266.06          0.0       organic  2018   \n",
       "18245    13655.49    13401.93      253.56          0.0       organic  2018   \n",
       "18246    16762.57    16510.32      252.25          0.0       organic  2018   \n",
       "18247    12341.48    12114.81      226.67          0.0       organic  2018   \n",
       "\n",
       "                 region  AveragePriceNextWeek  \n",
       "0                Albany                  1.24  \n",
       "1                Albany                  1.17  \n",
       "2                Albany                  1.06  \n",
       "3                Albany                  0.99  \n",
       "4                Albany                  0.99  \n",
       "...                 ...                   ...  \n",
       "18243  WestTexNewMexico                  1.57  \n",
       "18244  WestTexNewMexico                  1.54  \n",
       "18245  WestTexNewMexico                  1.56  \n",
       "18246  WestTexNewMexico                  1.56  \n",
       "18247  WestTexNewMexico                  1.62  \n",
       "\n",
       "[18141 rows x 14 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hastarget = create_lag_feature(df_sort, \"AveragePrice\", +1, [\"region\", \"type\"], \"AveragePriceNextWeek\", clip=True)\n",
    "df_hastarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict `AveragePriceNextWeek`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.4 `AveragePrice` baseline \n",
    "rubric={autograde:4}\n",
    "\n",
    "Soon we will want to build some models to forecast the average avocado price a week in advance. Before we start with any ML though, let's try a baseline. Previously we used `DummyClassifier` or `DummyRegressor` as a baseline. This time, we'll do something else as a baseline: we'll assume the price stays the same from this week to next week. So, we'll set our prediction of \"AveragePriceNextWeek\" exactly equal to \"AveragePrice\", assuming no change. That is kind of like saying, \"If it's raining today then I'm guessing it will be raining tomorrow\". This simplistic approach will not get a great score but it's a good starting point for reference. If our model does worse that this, it must not be very good. \n",
    "\n",
    "Using this baseline approach, what $R^2$ do you get on the train and test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_actual = df_train[\"AveragePriceNextWeek\"]\n",
    "y_train_baseline = df_train[\"AveragePrice\"]\n",
    "\n",
    "y_test_actual = df_test[\"AveragePriceNextWeek\"]\n",
    "y_test_baseline = df_test[\"AveragePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8285800937261841"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_r2 = r2_score(y_train_actual, y_train_baseline)\n",
    "train_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631780188583048"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_r2 = r2_score(y_test_actual, y_test_baseline)\n",
    "test_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1.4</pre></strong> passed! 🙌</p>"
      ],
      "text/plain": [
       "q1.4 results: All test cases passed!"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Forecasting average avocado price\n",
    "rubric={points:10}\n",
    "\n",
    "Now that the baseline is done, let's build some models to forecast the average avocado price a week later. Experiment with a few approachs for encoding the date. Justify the decisions you make. Which approach worked best? Report your test score and briefly discuss your results.\n",
    "\n",
    "Benchmark: you should be able to achieve $R^2$ of at least 0.79 on the test set. I got to 0.80, but not beyond that. Let me know if you do better!\n",
    "\n",
    "Note: because we only have 2 splits here, we need to be a bit wary of overfitting on the test set. Try not to test on it a ridiculous number of times. If you are interested in some proper ways of dealing with this, see for example sklearn's [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), which is like cross-validation for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to experiment with 4 approaches: default model without encoding the date, model encoding date as a number (Days since split_date: September 25 2017), model encoding date as a categorical variable (One Hot Encoding by Month), and model using lag-based features for AveragePrice (Average Price last week, Average Price 2 weeks ago, Average Price 3 weeks ago)\n",
    "\n",
    "The best approach was the model encoding date as a categorical variable (One Hot Encoding by Month) with a test score of 0.8. This model had the highest test score as compared to all 3 other models with a test score of 0.79 and 0.78. This indicates that this model is most likely to generalize the best on new unseen data (e.g. test data) as compared to the other models. This makes sense as ridge is a linear model that can only learn a linear function of the month of the year (encoded as categorical variable). The one-hot encoding of date feature transforms the cyclic temporal feature into a format where its impact on the target variable can be independently and linearly modelled, enabling the Ridge linear model to capture and use these cyclic patterns.\n",
    "\n",
    "The worst approach was the model encoding date as a number (Days since split_date: September 25 2017) with a test score of 0.78, which is worse than the default model without encoding the date (test score = 0.79). This is expected as the Ridge linear model is not able to capture the periodic/cyclic (non-linear) pattern in the data given that the encoded date feature is numeric.\n",
    "\n",
    "The model using lag-based features for AveragePrice performed similarly to the default model with a test score of 0.79 but had the highest train score of 0.86. Since the test score is not the highest, this model is not the best model. Furthermore, having the highest train score indicates that this model has the highest chance of overfitting on the train data, which further prevents it from being the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from Lecture 19\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "\n",
    "    all_features = set(numeric_features + categorical_features + drop_features + target)\n",
    "    if set(train_df.columns) != all_features:\n",
    "        print(\"Missing columns\", set(train_df.columns) - all_features)\n",
    "        print(\"Extra columns\", all_features - set(train_df.columns))\n",
    "        raise Exception(\"Columns do not match\")\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "    )\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (numeric_transformer, numeric_features),\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(train_df)\n",
    "    ohe_feature_names = (\n",
    "        preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "    new_columns = numeric_features + ohe_feature_names\n",
    "\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(test_df), index=test_df.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = train_df[\"AveragePriceNextWeek\"]\n",
    "    y_test = test_df[\"AveragePriceNextWeek\"]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"AveragePrice\",\n",
    "    \"Total Volume\",\n",
    "    \"4046\",\n",
    "    \"4225\",\n",
    "    \"4770\",\n",
    "    \"Total Bags\",\n",
    "    \"Small Bags\",\n",
    "    \"Large Bags\",\n",
    "    \"XLarge Bags\",\n",
    "    \"year\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \"type\",\n",
    "    \"region\",\n",
    "]\n",
    "drop_features = [\"Date\"]\n",
    "target = [\"AveragePriceNextWeek\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from Lecture 19\n",
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adapted from Lecture 19\n",
    "def score_lr_print_coeff(preprocessor, train_df, y_train, test_df, y_test, X_train_enc):\n",
    "    lr_pipe = make_pipeline(preprocessor, Ridge())\n",
    "    lr_pipe.fit(train_df, y_train)\n",
    "    print(\"Train score: {:.2f}\".format(lr_pipe.score(train_df, y_train)))\n",
    "    print(\"Test score: {:.2f}\".format(lr_pipe.score(test_df, y_test)))\n",
    "    lr_coef = pd.DataFrame(\n",
    "        data=lr_pipe.named_steps[\"ridge\"].coef_.flatten(),\n",
    "        index=X_train_enc.columns,\n",
    "        columns=[\"Coef\"],\n",
    "    )\n",
    "    return lr_coef.sort_values(by=\"Coef\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.328682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.087368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.084933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_NewYork</th>\n",
       "      <td>0.067025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_organic</th>\n",
       "      <td>0.049510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.045930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_conventional</th>\n",
       "      <td>-0.049510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.065811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.065815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.075620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.328682\n",
       "region_SanFrancisco         0.087368\n",
       "region_HartfordSpringfield  0.084933\n",
       "region_NewYork              0.067025\n",
       "type_organic                0.049510\n",
       "...                              ...\n",
       "region_Denver              -0.045930\n",
       "type_conventional          -0.049510\n",
       "region_DallasFtWorth       -0.065811\n",
       "region_SouthCentral        -0.065815\n",
       "region_Houston             -0.075620\n",
       "\n",
       "[66 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default model without encoding the date\n",
    "score_lr_print_coeff(preprocessor, df_train, y_train, df_test, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.78\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.324498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.091838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.089592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_NewYork</th>\n",
       "      <td>0.070944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Philadelphia</th>\n",
       "      <td>0.052258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.047782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_conventional</th>\n",
       "      <td>-0.052177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.069429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.070072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.079556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.324498\n",
       "region_SanFrancisco         0.091838\n",
       "region_HartfordSpringfield  0.089592\n",
       "region_NewYork              0.070944\n",
       "region_Philadelphia         0.052258\n",
       "...                              ...\n",
       "region_Denver              -0.047782\n",
       "type_conventional          -0.052177\n",
       "region_DallasFtWorth       -0.069429\n",
       "region_SouthCentral        -0.070072\n",
       "region_Houston             -0.079556\n",
       "\n",
       "[67 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from Lecture 19\n",
    "# Model encoding date as a number (Days since split_date: September 25 2017)\n",
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]\n",
    "\n",
    "first_day = df_train[\"Date\"].min()\n",
    "\n",
    "df_train = df_train.assign(\n",
    "    Days_since=df_train[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")\n",
    "df_test = df_test.assign(\n",
    "    Days_since=df_test[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")\n",
    "\n",
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    numeric_features + [\"Days_since\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")\n",
    "\n",
    "score_lr_print_coeff(preprocessor, df_train, y_train, df_test, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Adapted from Lecture 19\n",
    "# Model encoding date as a categorical variable (One Hot Encoding by Month)\n",
    "\n",
    "df_train = df_hastarget[df_hastarget[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget[df_hastarget[\"Date\"] >  split_date]\n",
    "\n",
    "df_train = df_train.assign(\n",
    "    Month=df_train[\"Date\"].apply(lambda x: x.month_name())\n",
    ") \n",
    "df_test = df_test.assign(Month=df_test[\"Date\"].apply(lambda x: x.month_name()))\n",
    "\n",
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    df_train, df_test, \n",
    "    numeric_features, \n",
    "    categorical_features + [\"Month\"], \n",
    "    drop_features,\n",
    "    target\n",
    ")\n",
    "\n",
    "month_df = score_lr_print_coeff(preprocessor, df_train, y_train, df_test, y_test, X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.86\n",
      "Test score: 0.79\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AveragePrice</th>\n",
       "      <td>0.243340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveragePrice_lag1</th>\n",
       "      <td>0.054982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SanFrancisco</th>\n",
       "      <td>0.050740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_HartfordSpringfield</th>\n",
       "      <td>0.043354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AveragePrice_lag3</th>\n",
       "      <td>0.038736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Denver</th>\n",
       "      <td>-0.025660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_conventional</th>\n",
       "      <td>-0.026386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_DallasFtWorth</th>\n",
       "      <td>-0.035993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_SouthCentral</th>\n",
       "      <td>-0.036761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>region_Houston</th>\n",
       "      <td>-0.040990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Coef\n",
       "AveragePrice                0.243340\n",
       "AveragePrice_lag1           0.054982\n",
       "region_SanFrancisco         0.050740\n",
       "region_HartfordSpringfield  0.043354\n",
       "AveragePrice_lag3           0.038736\n",
       "...                              ...\n",
       "region_Denver              -0.025660\n",
       "type_conventional          -0.026386\n",
       "region_DallasFtWorth       -0.035993\n",
       "region_SouthCentral        -0.036761\n",
       "region_Houston             -0.040990\n",
       "\n",
       "[69 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from Lecture 19\n",
    "# Model using lag-based features of AveragePrice (Average Price last week, Average Price 2 weeks ago, Average Price 3 weeks ago)\n",
    "\n",
    "df_hastarget_modified = create_lag_feature(df_hastarget, \"AveragePrice\", -1, [\"region\", \"type\"], \"AveragePrice_lag1\", clip=True)\n",
    "df_hastarget_modified = create_lag_feature(df_hastarget_modified, \"AveragePrice\", -2, [\"region\", \"type\"], \"AveragePrice_lag2\", clip=True)\n",
    "df_hastarget_modified = create_lag_feature(df_hastarget_modified, \"AveragePrice\", -3, [\"region\", \"type\"], \"AveragePrice_lag3\", clip=True)\n",
    "\n",
    "df_train = df_hastarget_modified[df_hastarget_modified[\"Date\"] <= split_date]\n",
    "df_test  = df_hastarget_modified[df_hastarget_modified[\"Date\"] >  split_date]\n",
    "\n",
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    numeric_features\n",
    "    + [\"AveragePrice_lag1\", \"AveragePrice_lag2\", \"AveragePrice_lag3\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")\n",
    "\n",
    "score_lr_print_coeff(\n",
    "    preprocessor, df_train, y_train, df_test, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Very short answer questions\n",
    "\n",
    "Each question is worth 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Time series\n",
    "\n",
    "rubric={points:4}\n",
    "\n",
    "The following questions pertain to Lecture 19 on time series data:\n",
    "\n",
    "1. Sometimes a time series has missing time points or, worse, time points that are unequally spaced in general. Give an example of a real world situation where the time series data would have unequally spaced time points.\n",
    "2. In class we discussed two approaches to using temporal information: encoding the date as one or more features, and creating lagged versions of features. Which of these (one/other/both/neither) two approaches would struggle with unequally spaced time points? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. An example of such a time series can be found in healthcare, particularly when monitoring patients with chronic conditions. This scenario can lead to time series data with unevenly spaced time points, since patient check-ups are often scheduled based on individual needs, health status changes, or the discretion of healthcare providers. Some patients may have more frequent appointments during critical periods, while others may follow a less frequent schedule during stable periods. This personalized approach to patient care results in a time series dataset where observations are not uniformly distributed.\n",
    "\n",
    "2. The approach of creating lagged versions of features would likely struggle more with unequally spaced time points. Lagged features involve creating time-shifted versions of existing features, aligning them with previous time points. However, when time points are unevenly spaced and arbitrary, the fixed lag approach may not capture the temporal dynamics accurately. The gaps between observations could lead to situations where the lagged features do not align appropriately with the underlying temporal patterns, potentially resulting in misleading or incomplete representations of the time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Survival analysis\n",
    "rubric={points:6}\n",
    "\n",
    "The following questions pertain to Lecture 20 on survival analysis. We'll consider the use case of customer churn analysis.\n",
    "\n",
    "1. What is the problem with simply labeling customers are \"churned\" or \"not churned\" and using standard supervised learning techniques?\n",
    "2. Consider customer A who just joined last week vs. customer B who has been with the service for a year. Who do you expect will leave the service first: probably customer A, probably customer B, or we don't have enough information to answer?\n",
    "3. If a customer's survival function is almost flat during a certain period, how do we interpret that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. One of the main problems with labeling customers as \"churned\" or \"not churned\" and carrying out a standard supervised learning technique, such as binary classification, is that this kind of data is often collected at a certain point in time, and thus we would be predicting whether a customer would churn at that particular point. Therefore, any analysis that we perform will be based on data that is not exactly accurate and that likely includes major class imbalance, which in turn leads to bias in the model and a poor prediction of whatever class is the minority.\n",
    "\n",
    "2. I believe the customer that is most likely to leave the service first is customer B. There are a few reasons for that: firstly, when you just join a service, you often have a month of free trial and you are still learning whether you'd like to keep the service or not. Generally, the longer one has been with a service, the more likely they are to churn based on probability. So customer B, who despite having been loyal to the service for a year, is statistically more likely to leave than customer A that just joined.\n",
    "\n",
    "3. Considering that the survival function represents the probability that an event (such as customer churn) has not occurred by a given time, we can say that if the function is almost flat during a certain period, it suggests that the probability of the event occurring remains relatively constant or low during that time interval. Furthermore, it can also indicate how stable the behaviour of that customer is - if the curve is flat, it suggests that customers are not showing a significant propensity to churn, and the factors influencing churn are relatively constant or well-managed. This stability can arise from various scenarios, from customer loyalty and genuine enjoyment of the service to long contracts that force a customer to remain with the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "name": "_merged",
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.4": {
     "name": "q1.4",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not train_r2 is None, 'Are you using the correct variable name?'\n>>> assert not test_r2 is None, 'Are you using the correct variable name?'\n>>> assert sha1(str(round(train_r2, 3)).encode('utf8')).hexdigest() == 'b1136fe2a8918904393ab6f40bfb3f38eac5fc39', 'Your training score is not correct. Are you using the right features?'\n>>> assert sha1(str(round(test_r2, 3)).encode('utf8')).hexdigest() == 'cc24d9a9b567b491a56b42f7adc582f2eefa5907', 'Your test score is not correct. Are you using the right features?'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
