{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw3.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 3: Preprocessing \n",
    "### Associated lectures: [Lectures 4, 5, 6](https://github.com/UBC-CS/cpsc330-2023W1/tree/main/lectures) \n",
    "\n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points:6}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 4. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from ‚Äú1‚Äù will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb.\n",
    "\n",
    "\n",
    "\n",
    "_Note: The assignments will get gradually more open-ended as we progress through the course. In many cases, there won't be a single correct solution. Sometimes you will have to make your own choices and your own decisions (for example, on what parameter values to use when they are not explicitly provided in the instructions). Use your own judgment in such cases and justify your choices, if necessary._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3247a4b883a670c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "A crucial step when using machine learning algorithms on real-world datasets is preprocessing. This homework will give you some practice of data preprocessing and building a supervised machine learning pipeline on a real-world dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Introducing the dataset\n",
    "<hr>\n",
    "\n",
    "In this lab, you will be working on [the adult census dataset](https://www.kaggle.com/uciml/adult-census-income#). Download the CSV and save it as `adult.csv` under the data folder in this homework folder. \n",
    "\n",
    "This is a classification dataset and the classification task is to predict whether income exceeds 50K per year or not based on the census data. You can find more information on the dataset and features [here](http://archive.ics.uci.edu/ml/datasets/Adult).\n",
    "\n",
    "The starter code below loads the data CSV (assuming that it is saved as `adult.csv` under the data folder). \n",
    "\n",
    "_Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_df = pd.read_csv(\"data/adult.csv\")\n",
    "census_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 1.1 Data splitting \n",
    "rubric={autograde:2}\n",
    "\n",
    "To prevent any violation of the golden rule, the initial step before proceeding with any task is to split the data.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into `train_df` (40%) and `test_df` (60%) with `random_state = 123`. Keep the target column (`income`) in the splits so that we can use it in the exploratory data analysis.  \n",
    "\n",
    "_Usually having more data for training is a good idea. But here I'm using 40%/60% split because running cross-validation with this dataset can take a while on a modest laptop. A smaller training data means it won't take too long to train the model on your laptop. A side advantage of this would be that with a bigger test split, we'll have a more reliable estimate of the model performance!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_df = None\n",
    "test_df = None\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    census_df, test_size=0.6, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1.1</pre></strong> passed! üôå</p>"
      ],
      "text/plain": [
       "q1.1 results: All test cases passed!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2: Exploratory data analysis (EDA) <a name=\"2\"></a> \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine our `train_df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90       NaN   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "5   34   Private  216864       HS-grad              9       Divorced   \n",
       "6   38   Private  150601          10th              6      Separated   \n",
       "\n",
       "        occupation   relationship   race     sex  capital.gain  capital.loss  \\\n",
       "0              NaN  Not-in-family  White  Female             0          4356   \n",
       "1  Exec-managerial  Not-in-family  White  Female             0          4356   \n",
       "4   Prof-specialty      Own-child  White  Female             0          3900   \n",
       "5    Other-service      Unmarried  White  Female             0          3770   \n",
       "6     Adm-clerical      Unmarried  White    Male             0          3770   \n",
       "\n",
       "   hours.per.week native.country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              18  United-States  <=50K  \n",
       "4              40  United-States  <=50K  \n",
       "5              45  United-States  <=50K  \n",
       "6              40  United-States  <=50K  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some missing values represented with a \"?\". Probably these were the questions not answered by some people during the census.  Usually `.describe()` or `.info()` methods would give you information on missing values. But here, they won't pick \"?\" as missing values because they are encoded as strings instead of an actual NaN in Python. So let's replace them with `np.nan` before we carry out EDA. If you do not do it, you'll encounter an error later on when you try to pass this data to a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13024, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.replace(\"?\", np.nan)\n",
    "test_df = test_df.replace(\"?\", np.nan)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>216864</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>150601</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>3770</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32554</th>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>116138</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>Taiwan</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>310152</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13024 rows √ó 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt     education  education.num      marital.status  \\\n",
       "0       90       NaN   77053       HS-grad              9             Widowed   \n",
       "1       82   Private  132870       HS-grad              9             Widowed   \n",
       "4       41   Private  264663  Some-college             10           Separated   \n",
       "5       34   Private  216864       HS-grad              9            Divorced   \n",
       "6       38   Private  150601          10th              6           Separated   \n",
       "...    ...       ...     ...           ...            ...                 ...   \n",
       "32554   32   Private  116138       Masters             14       Never-married   \n",
       "32556   22   Private  310152  Some-college             10       Never-married   \n",
       "32557   27   Private  257302    Assoc-acdm             12  Married-civ-spouse   \n",
       "32559   58   Private  151910       HS-grad              9             Widowed   \n",
       "32560   22   Private  201490       HS-grad              9       Never-married   \n",
       "\n",
       "            occupation   relationship                race     sex  \\\n",
       "0                  NaN  Not-in-family               White  Female   \n",
       "1      Exec-managerial  Not-in-family               White  Female   \n",
       "4       Prof-specialty      Own-child               White  Female   \n",
       "5        Other-service      Unmarried               White  Female   \n",
       "6         Adm-clerical      Unmarried               White    Male   \n",
       "...                ...            ...                 ...     ...   \n",
       "32554     Tech-support  Not-in-family  Asian-Pac-Islander    Male   \n",
       "32556  Protective-serv  Not-in-family               White    Male   \n",
       "32557     Tech-support           Wife               White  Female   \n",
       "32559     Adm-clerical      Unmarried               White  Female   \n",
       "32560     Adm-clerical      Own-child               White    Male   \n",
       "\n",
       "       capital.gain  capital.loss  hours.per.week native.country income  \n",
       "0                 0          4356              40  United-States  <=50K  \n",
       "1                 0          4356              18  United-States  <=50K  \n",
       "4                 0          3900              40  United-States  <=50K  \n",
       "5                 0          3770              45  United-States  <=50K  \n",
       "6                 0          3770              40  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "32554             0             0              11         Taiwan  <=50K  \n",
       "32556             0             0              40  United-States  <=50K  \n",
       "32557             0             0              38  United-States  <=50K  \n",
       "32559             0             0              40  United-States  <=50K  \n",
       "32560             0             0              20  United-States  <=50K  \n",
       "\n",
       "[13024 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"?\" symbols are now replaced with NaN values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.1 `describe()` method\n",
    "rubric={autograde:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Examine the output of `train_df.describe()` with `include='all'` argument and store it in a variable called `census_summary`.\n",
    "2. What is the highest hours per week someone reported? Store it in a variable called `max_hours_per_week`.\n",
    "3. What is the most frequently occurring occupation in this dataset? Store it in a variable called `most_freq_occupation`.\n",
    "4. Store the column names of the columns with missing values as a list in a variable called `missing_vals_cols`. \n",
    "5. Store the column names of all numeric-looking columns as a list in a variable called `numeric_cols`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "census_summary = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_hours_per_week = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max_hours_per_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_occupation = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_vals_cols = None\n",
    "numeric_cols = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sorting the lists for the autograder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmissing_vals_cols\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m()\n\u001b[1;32m      3\u001b[0m numeric_cols\u001b[38;5;241m.\u001b[39msort()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sort'"
     ]
    }
   ],
   "source": [
    "# Sorting the lists for the autograder\n",
    "missing_vals_cols.sort()\n",
    "numeric_cols.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Visualizing features\n",
    "rubric={points:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. For each numeric feature in `numeric_cols` you identified above, visualize the histograms for <=50K and >50K classes. \n",
    "2. Write a sentence or two describing your observations. \n",
    "\n",
    "> You can use the library of your choice for visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Identify transformations to apply\n",
    "rubric={points:13}\n",
    "\n",
    "Before passing this data to a machine learning model, we need to apply some transformations on different features. Below we are providing possible transformations which can be applied on each column in `census_df`.  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Write your justification or explanation for each row in the explanation column. An example explanation is given for the age column. \n",
    "\n",
    "> Note: This question is a bit open-ended. If you do not agree with the provided transformation, feel free to argue your case in the explanation. That said, in this assignment, go with the transformations provided below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.3\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "| Feature | Transformation | Explanation\n",
    "| --- | ----------- | ----- |\n",
    "| age | scaling |  A numeric feature with no missing values. It will be a good idea to apply scaling, as the range of values (17 to 90) is quite different compared to other numeric features.|\n",
    "| workclass | imputation, one-hot encoding | |\n",
    "| fnlwgt | drop |  |\n",
    "| education | ordinal encoding | |\n",
    "| education.num | drop | |\n",
    "| marital.status | one-hot encoding  | |\n",
    "| occupation | imputation, one-hot encoding  | |\n",
    "| relationship | one-hot encoding  | |\n",
    "| race | drop  |  |\n",
    "| sex | one-hot encoding with \"binary=True\" | |\n",
    "| capital.gain | scaling |  | \n",
    "| capital.loss | scaling |  |\n",
    "| hours.per.week | scaling | |\n",
    "| native.country | imputation, one-hot encoding | | \n",
    "\n",
    "\n",
    "_Points:_ 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 2.4 Identify feature types \n",
    "rubric={autograde:5}\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "1. Based on the types of transformations you want to apply on the features, identify different feature types and store them in the variables below as lists.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_2.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in the lists below.\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "ordinal_features = []\n",
    "binary_features = []\n",
    "drop_features = []\n",
    "target = \"income\"\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Sorting all the lists above for the autograder\n",
    "numeric_features.sort()\n",
    "categorical_features.sort()\n",
    "ordinal_features.sort()\n",
    "binary_features.sort()\n",
    "drop_features.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Baseline models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.1 Separating feature vectors and targets  \n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create `X_train`, `y_train`, `X_test`, `y_test` from `train_df` and `test_df`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 3.2 Dummy classifier\n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Carry out 5-fold cross-validation using `DummyClassifier` with `strategy=\"most_frequent\"` and `scikit-learn`'s `cross_validate` function with `return_train_scores=True` \n",
    "2. Store the results as a dataframe named `dummy_df` where each row corresponds to the results from a cross-validation fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "dummy_df = None \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3 Discussion\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Hopefully, you were able to run cross-validation with dummy classifier successfully in the question above. At this point, if you train [`sklearn`'s `SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model on `X_train` and `y_train` would it work? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_3.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Column Transformer\n",
    "<hr>\n",
    "\n",
    "In this dataset, we have different types of features: numeric features, an ordinal feature, categorical features, and a binary feature. We want to apply different transformations on different columns and therefore we need a column transformer. In this exercise, first, we'll define different transformations on different types of features and then will create a `scikit-learn`'s `ColumnTransformer`. For example, the code below creates a `numeric_transformer` for numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercises below, you'll create transformers for other types of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.1 Preprocessing ordinal features\n",
    "rubric={autograde:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a transformer called `ordinal_transformer` for our ordinal features. \n",
    "\n",
    "> Ordering of some of the education levels is not obvious. Assume that \"HS-grad\" < \"Prof-school\" < \"Assoc-voc\" < \"Assoc-acdm\" < \"Some-college\" < \"Bachelors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_transformer = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.2 Preprocessing binary features\n",
    "rubric={autograde:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a transformer called `binary_transformer` for our binary features. \n",
    "\n",
    "> _Note that many popular datasets have sex as a feature where the possible values are male and female. This representation reflects how the data were collected and is not meant to imply that, for example, gender is binary._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "binary_transformer = None\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.3 Preprocessing categorical features\n",
    "rubric={autograde:4}\n",
    "\n",
    "In Exercises 2.1 and 2.3, we saw that there are 3 categorical features with missing values. So first we need to impute the missing values and then encode these features with one-hot encoding. For the purpose of this assignment, let's just have imputation as the first step for all categorical features even when they do not have missing values. This should be OK because if a feature doesn't have any missing value,  imputation won't be applied. \n",
    "\n",
    "If we want to apply more than one transformation on a set of features, we need to create a [`scikit-learn` `Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). For example, for categorical features we can create a `scikit-learn` `Pipeline` with first step as imputation and the second step as one-hot encoding. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a `sklearn` `Pipeline` using [`make_pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) called `categorical_transformer` for our categorical features with two steps: `SimpleImputer` for imputation with `strategy=\"constant\"` and `fill_value=\"missing\"` and `OneHotEncoder` with `handle_unknown=\"ignore\"` for one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "categorical_transformer = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### 4.4 Creating a column transformer. \n",
    "rubric={autograde:7}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Create a `sklearn` `ColumnTransformer` named `preprocessor` using [`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html) with the transformers defined in the previous exercises. Use the sequence below in the column transformer and add a \"drop\" step for the `drop_features` in the end.  \n",
    "    - `numeric_transformer`\n",
    "    - `ordinal_transformer`\n",
    "    - `binary_transformer`\n",
    "    - `categorical_transformer`\n",
    "2. Transform the data by calling `fit_transform` on the training set and save it as a dataframe in a variable called `transformed_df`. How many new columns have been created in the preprocessed data in comparison to the original `X_train`? Store the difference between the number of columns in `transformed_df` and `X_train` in a variable called `n_new_cols`.\n",
    "\n",
    "> You are not required to do this but optionally you can try to get column names of the transformed data and create the dataframe `transformed_df` with proper column names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_4.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = None\n",
    "n_new_cols = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Building models \n",
    "\n",
    "Now that we have preprocessed features, we are ready to build models. Below, I'm providing the function we used in class which returns mean cross-validation score along with standard deviation for a given model. Use it to keep track of your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "results_dict = {}  # dictionary to store all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores.iloc[i], std_scores.iloc[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'm showing an example where I call `mean_std_cross_val_scores` with `DummyClassifier`. The function calls `cross_validate` with the passed arguments and returns a series with mean cross-validation results and std of cross-validation. When you train new models, you can just add the results of these models in `results_dict`, which can be easily converted to a dataframe so that you can have a table with all your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(random_state = 123)\n",
    "pipe = make_pipeline(preprocessor, dummy)\n",
    "results_dict[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    pipe, X_train, y_train, cv=5, return_train_score=True\n",
    ")\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1 Trying different classifiers\n",
    "rubric={autograde:10}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. For each of the models in the starter code below: \n",
    "    - Define a pipeline with two steps: `preprocessor` from 4.4 and the model as your classifier. \n",
    "    - Carry out 5-fold cross-validation with the pipeline and get the mean cross-validation scores with std by calling the `mean_std_cross_val_scores` function above. \n",
    "    - Store the results in a dataframe called `income_pred_results_df` with the model names in the `models` dictionary below as the index and each row representing results returned by `mean_std_cross_val_scores` function above. In other words, `income_pred_results_df` should look similar to the `results_df` dataframe above with more rows for the models below. \n",
    "    \n",
    "> This might take a while to run. Be patient! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"decision tree\": DecisionTreeClassifier(random_state=123),\n",
    "    \"kNN\": KNeighborsClassifier(),\n",
    "    \"RBF SVM\": SVC(random_state=123),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "income_pred_results_df = None \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5.2 Discussion \n",
    "rubric={points:5}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Examine the train and validation accuracies and `fit` and `score` times for all the models in the results above and answer the questions below: \n",
    "1. Among the models, which one achieves the highest validation accuracy?\n",
    "2. Excluding the dummy model, which model exhibits the shortest 'fit' time? Similarly, which model boasts the quickest 'score' time?\n",
    "3. Among the models, which one shows the highest degree of overfitting and which one exhibits the least overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 5.3 Hyperparameter optimization\n",
    "rubric={points:10}\n",
    "\n",
    "In this exercise, you'll carry out hyperparameter optimization for the hyperparameter `C` of `SVC RBF` classifier. In practice, you'll carry out hyperparameter optimization for all different hyperparameters of the most promising classifiers. For the purpose of this assignment, we'll only do it for the `SVC` classifier with one hyperparameter, namely `C`. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. For each `C` value in the `param_grid` below: \n",
    "    - Create a pipeline object with two steps: preprocessor from 4.4 and `SVC` classifier with the `C` value.\n",
    "    - Carry out 5-fold cross validation with the pipeline.  \n",
    "    - Store the results in `results_dict` and display results as a pandas DataFrame. \n",
    "2. Which hyperparameter value seems to be performing the best? Store it in a variable called `best_C`. (Since this question is not autograded, please store the value directly as a number, something like `best_C = 0.001`, if `C = 0.001` is giving you the best CV score.) Is it different than the default value for the hyperparameter used by `scikit-learn`? \n",
    "\n",
    "> Note: Running this will take a while. Please be patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\"C\": np.logspace(-1, 2, 4)}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_5.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "best_C = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Exercise 6: Evaluating on the test set \n",
    "<hr>\n",
    "\n",
    "Now that we have a best performing model, it's time to assess our model on the set aside test set. In this exercise, you'll examine whether the results you obtained using cross-validation on the train set are consistent with the results on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 6.1 Scoring on the unseen test set \n",
    "rubric={autograde:4}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create a pipeline named `final_pipeline` with the preprocessor from 4.4 as the first step and the best performing SVC model from 5.3 as the second step (i.e. `C=best_C`). \n",
    "2. Train `final_pipeline` on the entire training set `X_train` and `y_train`. \n",
    "3. Score `final_pipeline` on `X_test` and `y_test` and store the score in a variable called `test_score`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_6.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "final_pipeline = None\n",
    "test_score = None\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 7.1 kNNs toy example\n",
    "rubric={points:3}\n",
    "\n",
    "Suppose you want to get predictions using the $k$-nearest neighbour algorithm on the toy dataset below. \n",
    "\n",
    "$$ X = \\begin{bmatrix}5 & 2\\\\4 & -2\\\\  2 & 2\\\\ 10 & 10\\\\ 9 & -1\\\\ 9& 9\\end{bmatrix}, \\quad y = \\begin{bmatrix}0\\\\0\\\\1\\\\1\\\\1\\\\2\\end{bmatrix}.$$\n",
    "\n",
    "Suppose you are given a new data point $x=\\begin{bmatrix} 0 & 0\\end{bmatrix}$.  \n",
    "\n",
    "1. What would the model predict when $k=1$?\n",
    "2. What would the model predict when $k=3$?\n",
    "3. What would the model predict when $k=3$ if we were doing regression rather than classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_7.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 7.2 Preprocessing\n",
    "rubric={points:8}\n",
    "\n",
    "1. What is the problem with calling `fit_transform` on your test data with `StandardScaler`?\n",
    "2. Why is it important to follow the Golden Rule? If you violate it, will that give you a worse classifier?\n",
    "3. What are two advantages of using sklearn Pipelines? \n",
    "4. When is it appropriate to use sklearn `ColumnTransformer`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "Solution_7.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Congratulations on finishing the homework! This was a tricky one but I hope you are feeling good after working on it. You are now ready to build a simple supervised machine learning pipeline on real-world datasets! Well done :clap:! \n",
    "\n",
    "![](img/eva-well-done.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not train_df is None and not test_df is None, \"Please use the provided variables.\"\n>>> assert train_df.shape == (13024, 15), \"The dimensions of the training set are incorrect\"\n>>> assert test_df.shape == (19537, 15), \"The dimensions of the test set are incorrect\"\n>>> assert train_df.loc[12846][['age', 'education', 'occupation', 'capital.loss']].tolist() == [49, 'Some-college', 'Craft-repair', 0], \"Are you using the provided random state?\"\n>>> assert not 20713 in train_df.index, 'Are you using the provided random state?' \n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.1": {
     "name": "q2.1",
     "points": [
      1,
      1,
      1,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # Task 1\n>>> assert isinstance(census_summary, pd.DataFrame), \"census_summary dataframe is not created\"\n>>> assert census_summary.shape == (11, 15), \"census_summary shape is incorrect. Probably you are not including all columns\"\n>>> assert census_summary.loc['min']['age'] == 17.0, \"census_summary dataframe is incorrect\"\n>>> assert sha1(str(census_summary.loc['top']['occupation']).encode('utf8')).hexdigest() == '97165f50eddb0d28a382b0366274e2fe38505644', \"census_summary dataframe is incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 2\n>>> assert (sha1(str(float(max_hours_per_week)).encode('utf8')).hexdigest() == \"3359de52c8ae993fe0f8fe9c5168a0065bd3c7a4\"), \"max_hours_per_week are incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 3\n>>> assert (sha1(str(most_freq_occupation).encode('utf8')).hexdigest() == \"97165f50eddb0d28a382b0366274e2fe38505644\"), \"most_freq_occupation is incorrect\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 4\n>>> assert (sha1(str(missing_vals_cols).encode('utf8')).hexdigest() == \"6bc5e13d4d66b306e52701ee9a1e5e21bf19aeb0\"), \"Please use the exact column/feature name. Also, make sure the lists are sorted.\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Task 5\n>>> assert (sha1(str(numeric_cols).encode('utf8')).hexdigest() == \"615afaf5011128d641ab8a73289d57bd01a3ec37\"), \"Please use the exact column/feature name. Also, make sure the lists are sorted.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.4": {
     "name": "q2.4",
     "points": [
      5
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert (sha1(str(numeric_features).encode('utf8')).hexdigest() == \"71401cf60034fd69eee7398866359f612adf3e15\"), \"numeric_features list is not correct\"\n>>> assert (sha1(str(categorical_features).encode('utf8')).hexdigest() == \"af1a4022c0362405678be5c3a6735578a8c0069f\"), \"categorical_features list is not correct\"\n>>> assert (sha1(str(ordinal_features).encode('utf8')).hexdigest() == \"95b86602c44211f3ad662bb58b8e53d024106d05\"), \"ordinal_features list is not correct\"\n>>> assert (sha1(str(binary_features).encode('utf8')).hexdigest() == \"d4b7aa4c56ac2f98e6ac9cec7768484b415b7337\"), \"binary_features list is not correct\"\n>>> assert (sha1(str(drop_features).encode('utf8')).hexdigest() == \"62aab57d42c54be3dfd3c55020e5a167ca1a84c3\"), \"drop_features list is not correct\"\n>>> assert (sha1(str(target).encode('utf8')).hexdigest() == \"0f613350b66e64d92ef21bc4dcdbf8996cb4edf0\"), \"target variable is not set correctly\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": [
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not X_train is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not y_train is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not X_test is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert not y_test is None, \"Your answer does not exist. Have you passed in the correct variable?\"\n>>> assert X_train.shape == (13024, 14), \"The dimensions of X_train are incorrect\"\n>>> assert y_train.shape == (13024, ), \"The dimensions of y_train are incorrect. Are you splitting correctly?\"\n>>> assert X_test.shape == (19537,14), \"The dimensions of X_test are incorrect. Are you splitting correctly? Are you using single brackets?\"\n>>> assert y_test.shape == (19537,), \"The dimensions of y_test are incorrect. Are you splitting correctly? Are you using single brackets?\"\n>>> assert 'income' not in list(X_train.columns), \"Make sure the target variable is not part of your X dataset.\"\n>>> assert 'income' not in list(X_test.columns), \"Make sure the target variable is not part of your X dataset.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": [
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not dummy_df is None, \"Have you used the correct variable to store the results?\"\n>>> assert sorted(list(dummy_df.columns)) == ['fit_time','score_time','test_score','train_score'], \"Your solution contains incorrect columns.\"\n>>> assert dummy_df.shape == (5,4), \"Are you carrying out 5-fold cross-validation and passing return_train_score=True?\"\n>>> assert np.isclose(round(dummy_df['test_score'].mean(),3), 0.758), \"The test scores seem wrong. Are you calling the cross_validate correctly?\"\n>>> assert np.isclose(round(dummy_df['train_score'].mean(),3), 0.758), \"The train scores seem wrong. Are you calling the cross_validate correctly?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.1": {
     "name": "q4.1",
     "points": [
      5
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not ordinal_transformer is None, \"Are you using the correct variable name?\"\n>>> assert type(ordinal_transformer.get_params()['categories'][0]) is list, \"Are you passing education levels as a list of lists?\"\n>>> assert ordinal_transformer.get_params()['dtype'] == int, \"Please set the dtype to int\"\n>>> assert (sha1(str(ordinal_transformer.get_params()['categories'][0]).encode('utf8')).hexdigest() == \"893a03d114b2af09b53247866c6eea54ebfd090f\") or (sha1(str(ordinal_transformer.get_params()['categories'][0]).encode('utf8')).hexdigest() == \"81059b8bebc9ddb03d61bf07cfd9b9b6b0da288e\"), \"Are you using the correct order of education levels? (Ascending or descending shouldn't matter.)\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.2": {
     "name": "q4.2",
     "points": [
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not binary_transformer is None, \"Are you using the correct variable name?\"\n>>> assert binary_transformer.get_params()['drop'] == 'if_binary', \"Are you passing `drop=if_binary`?\"\n>>> assert binary_transformer.get_params()['dtype'] == int, \"Please set the dtype to int\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.3": {
     "name": "q4.3",
     "points": [
      4
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert not categorical_transformer is None, \"Are you using the correct variable name?\"\n>>> assert type(categorical_transformer) is Pipeline, \"Are you creating a scikit-learn Pipeline?\"\n>>> assert len(categorical_transformer.get_params()['steps']) == 2, \"Are you creating a pipeline with two steps?\"\n>>> assert categorical_transformer.get_params()['simpleimputer__strategy'] == 'constant', \"Are you passing strategy=constant in the SimpleImputer?\"\n>>> assert categorical_transformer.get_params()['simpleimputer__fill_value'] == 'missing', \"Are you passing fill_value='missing' in the SimpleImputer?\"\n>>> assert categorical_transformer.get_params()['onehotencoder__handle_unknown'] == 'ignore', \"Are you passing handle_unknown = 'ignore' argument to your OHE?\"\n>>> assert categorical_transformer.get_params()['onehotencoder__sparse_output'] == False, \"Are you creating a sparase matrix for OHE?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4.4": {
     "name": "q4.4",
     "points": [
      5,
      1,
      1
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # task 1\n>>> assert not preprocessor is None, \"Are you using the correct variable name?\"\n>>> assert len(preprocessor.get_params()['transformers']) in range(4,6,1), \"Have you included all the transformers?\"\n>>> assert 'onehotencoder' in preprocessor.get_params().keys(), 'Either the categorical_transformer or binary_transformer is not included.'\n>>> assert 'standardscaler' in preprocessor.get_params().keys(), 'numeric_transformer is not included.'\n>>> assert 'ordinalencoder' in preprocessor.get_params().keys(), 'ordinal_transformer is not included.'\n>>> assert 'drop' in preprocessor.get_params().keys(), 'drop features step is not included.'\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # task 2\n>>> assert not transformed_df is None, \"Are you using the correct variable name?\"\n>>> assert sha1(str(transformed_df.shape).encode('utf8')).hexdigest() == 'a0521f0cdbcd77cd213e7d1a3cfc13c1c7c92a6e', \"The shape of the transformed data is incorrect.\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not n_new_cols is None, \"Are you using the correct variable name?\"\n>>> assert sha1(str(n_new_cols).encode('utf8')).hexdigest() == 'b7103ca278a75cad8f7d065acda0c2e80da0b7dc', \"The number of new columns (n_new_cols) is incorrect.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5.1": {
     "name": "q5.1",
     "points": [
      2,
      4,
      4
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(income_pred_results_df, pd.DataFrame), \"`income_pred_results_df` dataframe is not created\"\n>>> assert len(income_pred_results_df.columns.values.tolist()) == 4, \"Your results contain the wrong number of columns. Did you use `return_train_score=True`?\"\n>>> assert income_pred_results_df.columns.values.tolist() == ['fit_time', 'score_time', 'test_score', 'train_score'], \"Your result column names look wrong. Did you use `mean_std_cross_val_scores` for cross validation?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert 'decision tree' in income_pred_results_df.index, \"Did you include the model `decision tree` in your results?\"\n>>> assert 'kNN' in income_pred_results_df.index, \"Did you include the model `kNN` in your results?\"\n>>> assert 'RBF SVM' in income_pred_results_df.index, \"Did you include the model `RBF SVM` in your results?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> assert not isinstance(income_pred_results_df.at['decision tree', 'test_score'], np.ndarray), \"Your results format seems wrong. Did you use `mean_std_cross_val_scores` for cross validation?\"\n>>> assert income_pred_results_df.at['decision tree', 'test_score'] == \"0.814 (+/- 0.012)\", \"Your decision tree `test_score` seems wrong. Did you use `mean_std_cross_val_scores` for cross validation?\"\n>>> assert income_pred_results_df.at['RBF SVM', 'train_score'] == \"0.855 (+/- 0.002)\", \"Your RBF SVM `train_score` seems wrong. Did you use `mean_std_cross_val_scores` for cross validation?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6.1": {
     "name": "q6.1",
     "points": [
      2,
      2
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> # task 1\n>>> assert not final_pipeline is None, \"Are you using the correct variable name?\"\n>>> assert not test_score is None, \"Are you using the correct variable name?\"\n>>> assert len(final_pipeline.named_steps) == 2, \"The final pipeline needs to have two steps: one for the preprocessor and one for SVC.\"\n>>> assert final_pipeline.n_features_in_ == 14, \"Make sure to pass the original X_train to fit\"\n>>> assert final_pipeline.named_steps['svc'].get_params()['C'] == best_C, \"Are you using the best C value from the previous exercise?\"\n",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # task 2\n>>> assert not test_score is None, \"Are you using the correct variable name?\"\n>>> assert np.isclose(round(test_score, 3), 0.85), \"The test score seems off\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "59ed430e69f497e5c6bdf6cb8dd692fd34850307131b5327e821a0fe7de503c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
