# Initialize Otter
import otter
grader = otter.Notebook("hw3.ipynb")


from hashlib import sha1
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


census_df = pd.read_csv("data/adult.csv")
census_df.shape


train_df = None
test_df = None

train_df, test_df = train_test_split(
    census_df, test_size=0.6, random_state=123
)


grader.check("q1.1")


train_df.sort_index().head()


train_df = train_df.replace("?", np.nan)
test_df = test_df.replace("?", np.nan)
train_df.shape


train_df.sort_index()


census_summary = train_df.describe(include='all')
census_summary


max_hours_per_week = census_summary['hours.per.week'].loc['max']
max_hours_per_week


type(max_hours_per_week)


most_freq_occupation = census_summary['occupation'].loc['top']
most_freq_occupation


missing_vals_cols = []
numeric_cols = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']

for column in train_df.columns:
    if train_df[column].isnull().any():
        missing_vals_cols.append(column)

missing_vals_cols


# Sorting the lists for the autograder
missing_vals_cols.sort()
numeric_cols.sort()


grader.check("q2.1")


for column in numeric_cols:
    train_df.groupby('income')[column].plot.hist(edgecolor = "white", alpha=0.7, legend=True)
    plt.xlabel(column)
    plt.title("Histogram of " + column)
    plt.show()


# Fill in the lists below.
# Fill in the lists below.
numeric_features = ['age', 'capital.gain', 'capital.loss', 'hours.per.week']
categorical_features = ['workclass', 'marital.status', 'occupation', 'relationship', 'native.country']
ordinal_features = ['education']
binary_features = ['sex']
drop_features = ['fnlwgt', 'education.num', 'race']
target = "income"


# Sorting all the lists above for the autograder
numeric_features.sort()
categorical_features.sort()
ordinal_features.sort()
binary_features.sort()
drop_features.sort()


grader.check("q2.4")


X_train = train_df.drop(columns=['income'])
y_train = train_df['income']
X_test = test_df.drop(columns=['income'])
y_test = test_df['income']


grader.check("q3.1")


dummy_df = None 

from sklearn.dummy import DummyClassifier

dummy = DummyClassifier(strategy="most_frequent")
scores = cross_validate(dummy, X_train, y_train, return_train_score=True)

dummy_df = pd.DataFrame(scores)
dummy_df


grader.check("q3.2")


numeric_transformer = StandardScaler()


ordinal_transformer = OrdinalEncoder(categories=[["Preschool",
                                                 "1st-4th",
                                                 "5th-6th",
                                                 "7th-8th",
                                                 "9th",
                                                 "10th",
                                                 "11th",
                                                 "12th",
                                                 "HS-grad",
                                                 "Prof-school",
                                                 "Assoc-voc",
                                                 "Assoc-acdm",
                                                 "Some-college",
                                                 "Bachelors",
                                                 "Masters",
                                                 "Doctorate"]],
                                    dtype = int)
ordinal_transformer.fit(X_train[["education"]])


grader.check("q4.1")


binary_transformer = OneHotEncoder(drop="if_binary", dtype=int)
binary_transformer.fit(X_train[["sex"]])


grader.check("q4.2")


categorical_transformer = make_pipeline(
    SimpleImputer(strategy="constant", fill_value="missing"), OneHotEncoder(handle_unknown="ignore", sparse_output=False))


grader.check("q4.3")


preprocessor = make_column_transformer(
    (numeric_transformer, numeric_features),
    (ordinal_transformer, ordinal_features),
    (binary_transformer, binary_features),
    (categorical_transformer, categorical_features),
    ('drop', drop_features)
)


transformed_df = preprocessor.fit_transform(X_train)
n_new_cols = len(transformed_df[0])-len(X_train.columns)


grader.check("q4.4")


results_dict = {}  # dictionary to store all the results


def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):
    """
    Returns mean and std of cross validation

    Parameters
    ----------
    model :
        scikit-learn model
    X_train : numpy array or pandas DataFrame
        X in the training data
    y_train :
        y in the training data

    Returns
    ----------
        pandas Series with mean scores from cross_validation
    """

    scores = cross_validate(model, X_train, y_train, **kwargs)

    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []

    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores.iloc[i], std_scores.iloc[i])))

    return pd.Series(data=out_col, index=mean_scores.index)


# Baseline model

from sklearn.dummy import DummyClassifier

dummy = DummyClassifier(random_state = 123)
pipe = make_pipeline(preprocessor, dummy)
results_dict["dummy"] = mean_std_cross_val_scores(
    pipe, X_train, y_train, cv=5, return_train_score=True
)
results_df = pd.DataFrame(results_dict).T
results_df


models = {
    "decision tree": DecisionTreeClassifier(random_state=123),
    "kNN": KNeighborsClassifier(),
    "RBF SVM": SVC(random_state=123),
}


for x in models:
    model = models[x]
    pipe = make_pipeline(preprocessor, model)
    results_dict[x] = mean_std_cross_val_scores(pipe, X_train, y_train, cv=5, return_train_score=True)

income_pred_results_df = pd.DataFrame(results_dict).T
income_pred_results_df


grader.check("q5.1")


param_grid = {"C": np.logspace(-1, 2, 4)}
param_grid


results_dict = {}

for i in range(len(param_grid["C"])):
    pipe = make_pipeline(preprocessor, SVC(C=(param_grid["C"][i])))
    results_dict[(param_grid["C"][i])] = mean_std_cross_val_scores(pipe, X_train, y_train, cv=5, return_train_score=True)

results_df = pd.DataFrame(results_dict).T
results_df


best_C = 100.0
best_C


final_pipeline = make_pipeline(preprocessor, SVC(C=best_C))

final_pipeline.fit(X_train, y_train)

test_score = final_pipeline.score(X_test, y_test)

test_score


grader.check("q6.1")
