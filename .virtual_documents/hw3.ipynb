# Initialize Otter
import otter
grader = otter.Notebook("hw3.ipynb")


from hashlib import sha1
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.dummy import DummyClassifier
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score, cross_validate, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier


census_df = pd.read_csv("data/adult.csv")
census_df.shape


train_df = None
test_df = None

train_df, test_df = train_test_split(
    census_df, test_size=0.6, random_state=123
)


grader.check("q1.1")


train_df.sort_index().head()


train_df = train_df.replace("?", np.nan)
test_df = test_df.replace("?", np.nan)
train_df.shape


train_df.sort_index()


census_summary = train_df.describe(include='all')
census_summary


max_hours_per_week = census_summary['hours.per.week'].loc['max']
max_hours_per_week


type(max_hours_per_week)


most_freq_occupation = census_summary['occupation'].loc['top']
most_freq_occupation


missing_vals_cols = []
numeric_cols = ['age','fnlwgt','education.num','capital.gain','capital.loss','hours.per.week']

for column in train_df.columns:
    if train_df[column].isnull().any():
        missing_vals_cols.append(column)

missing_vals_cols


# Sorting the lists for the autograder
missing_vals_cols.sort()
numeric_cols.sort()


grader.check("q2.1")


...


# Fill in the lists below.
numeric_features = []
categorical_features = []
ordinal_features = []
binary_features = []
drop_features = []
target = "income"

...


# Sorting all the lists above for the autograder
numeric_features.sort()
categorical_features.sort()
ordinal_features.sort()
binary_features.sort()
drop_features.sort()


grader.check("q2.4")


X_train = None
y_train = None
X_test = None
y_test = None

...


grader.check("q3.1")


dummy_df = None 

...


grader.check("q3.2")


numeric_transformer = StandardScaler()


ordinal_transformer = None

...


...


...


grader.check("q4.1")


binary_transformer = None
...


grader.check("q4.2")


categorical_transformer = None

...


grader.check("q4.3")


preprocessor = None

...


transformed_df = None
n_new_cols = None

...


grader.check("q4.4")


results_dict = {}  # dictionary to store all the results


def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):
    """
    Returns mean and std of cross validation

    Parameters
    ----------
    model :
        scikit-learn model
    X_train : numpy array or pandas DataFrame
        X in the training data
    y_train :
        y in the training data

    Returns
    ----------
        pandas Series with mean scores from cross_validation
    """

    scores = cross_validate(model, X_train, y_train, **kwargs)

    mean_scores = pd.DataFrame(scores).mean()
    std_scores = pd.DataFrame(scores).std()
    out_col = []

    for i in range(len(mean_scores)):
        out_col.append((f"%0.3f (+/- %0.3f)" % (mean_scores.iloc[i], std_scores.iloc[i])))

    return pd.Series(data=out_col, index=mean_scores.index)


# Baseline model

from sklearn.dummy import DummyClassifier

dummy = DummyClassifier(random_state = 123)
pipe = make_pipeline(preprocessor, dummy)
results_dict["dummy"] = mean_std_cross_val_scores(
    pipe, X_train, y_train, cv=5, return_train_score=True
)
results_df = pd.DataFrame(results_dict).T
results_df


models = {
    "decision tree": DecisionTreeClassifier(random_state=123),
    "kNN": KNeighborsClassifier(),
    "RBF SVM": SVC(random_state=123),
}


income_pred_results_df = None 
...


...


grader.check("q5.1")


param_grid = {"C": np.logspace(-1, 2, 4)}
param_grid


...


...


best_C = None

...


final_pipeline = None
test_score = None

...


grader.check("q6.1")
