# Initialize Otter
import otter
grader = otter.Notebook("hw4.ipynb")


from hashlib import sha1
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

plt.rcParams["font.size"] = 16

from sklearn.dummy import DummyClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import (
    GridSearchCV,
    cross_val_score,
    cross_validate,
    train_test_split,
)
from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.tree import DecisionTreeClassifier


class MyDummyClassifier:
    """
    A baseline classifier that predicts the most common class.
    The predicted probabilities come from the relative frequencies
    of the classes in the training data.

    This implementation only works when y only contains 0's and 1's.
    """

    prob_zero = 0
    prob_one = 0
    
    def fit(self, X, y):
        """
        Fit the Dummy Classifier to the training data.

        Parameters:
        - X (array-like, shape (n_samples, n_features)): Training data.
        - y (array-like, shape (n_samples,)): Target labels (0's and 1's).

        Returns:
        - None        
        """
        self.prob_one = np.mean(y)
        self.prob_zero = 1 - self.prob_one
        return None  
        
    def predict(self, X):
        """
        Predict the target labels for the input data.

        Parameters:
        - X (array-like, shape (n_samples, n_features)): Input data.

        Returns:
        - y_pred (array-like, shape (n_samples,)): Predicted target labels.
        """                
        predictions = np.zeros(X.shape[0]) # initializing with all predictions set to 0 
        if (self.prob_one > self.prob_zero):
            predictions = np.ones(X.shape[0])
        return predictions

    def predict_proba(self, X):
        """
        Predict class probabilities for the input data.

        Parameters:
        - X (array-like, shape (n_samples, n_features)): Input data.

        Returns:
        - probs (array-like, shape (n_samples, 2)): Predicted class probabilities.
          Column 0 corresponds to class 0, and column 1 corresponds to class 1.
        """        
        probs = np.zeros((X.shape[0], 2)) # initializing all probabilities set to 0. 
        probs[:, 0] = self.prob_zero
        probs[:, 1] = self.prob_one
        return probs  # Replace with your code

    def score(self, X, y):
        """
        Calculate the accuracy of the model on the input data.

        Parameters:
        - X (array-like, shape (n_samples, n_features)): Input data.
        - y (array-like, shape (n_samples,)): True target labels.

        Returns:
        - accuracy (float): Accuracy of the model on the input data.
        """        
        correct_pred = 0
        predictions = self.predict(X)
        for i in range(predictions.shape[0]):
            if predictions[i] == y[i]:
                correct_pred += 1   
        accuracy = correct_pred / y.shape[0]
        return accuracy 
    


# For testing, generate random data
n_train = 101
n_valid = 21
d = 5
X_train_dummy = np.random.randn(n_train, d)
X_valid_dummy = np.random.randn(n_valid, d)
y_train_dummy = np.random.randint(2, size=n_train)
y_valid_dummy = np.random.randint(2, size=n_valid)

my_dc = MyDummyClassifier()
sk_dc = DummyClassifier(strategy="prior")

my_dc.fit(X_train_dummy, y_train_dummy)
sk_dc.fit(X_train_dummy, y_train_dummy)

assert np.array_equal(my_dc.predict(X_train_dummy), sk_dc.predict(X_train_dummy))
assert np.array_equal(my_dc.predict(X_valid_dummy), sk_dc.predict(X_valid_dummy))


assert np.allclose(
    my_dc.predict_proba(X_train_dummy), sk_dc.predict_proba(X_train_dummy)
)
assert np.allclose(
    my_dc.predict_proba(X_valid_dummy), sk_dc.predict_proba(X_valid_dummy)
)


assert np.isclose(
    my_dc.score(X_train_dummy, y_train_dummy), sk_dc.score(X_train_dummy, y_train_dummy)
)
assert np.isclose(
    my_dc.score(X_valid_dummy, y_valid_dummy), sk_dc.score(X_valid_dummy, y_valid_dummy)
)


grader.check("q1")


tweets_df = pd.read_csv("data/realdonaldtrump.csv", index_col=0)
tweets_df.head()


tweets_df.shape


y = tweets_df["retweets"] > 10_000


X = tweets_df["content"]


# YOUR COMMENT HERE
countvec = CountVectorizer(stop_words="english")

# YOUR COMMENT HERE
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=123)

# YOUR COMMENT HERE
cv_score = cross_val_score(pipe, X_train, y_train).mean()

# YOUR COMMENT HERE
pipe = make_pipeline(countvec, lr)

# YOUR COMMENT HERE
lr = LogisticRegression(max_iter=1000, random_state=123)


# We're splitting the dataset into the training and tests splits with 60% of it being assigned to the test set.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=123)

# Creating the count vectorizer object, setting stop_words to english to avoid including common English conjunctions and articles into the bag of words representation
countvec = CountVectorizer(stop_words="english")

# Creating the logistic regression model object, setting max iterations to 1000 so that ... TODO
lr = LogisticRegression(max_iter=1000, random_state=123)

# Making the pipeline object with the first step as the count vectorizer and the second as the logistic regression
pipe = make_pipeline(countvec, lr)

# Calculating the mean cross validation score for the model
cv_score = cross_val_score(pipe, X_train, y_train).mean()


dummy_cv_score = None

dummy = DummyClassifier()
dummy_cv_score = cross_val_score(dummy, X_train, y_train).mean()

dummy_cv_score


grader.check("q2.2")


cv_scores = pd.DataFrame(cross_validate(pipe, X_train, y_train))['test_score']
cv_scores


pipe_lr = make_pipeline(CountVectorizer(stop_words="english"), LogisticRegression(max_iter=1000, random_state=123))
pipe_lr.fit(X_train, y_train);


tweet = None
prob = None

probas = pipe_lr.predict_proba(X_test).tolist()
test_df = pd.DataFrame(X_test)
max_proba = 0
tweet_index = None

for i in probas:
    proba = i[1]
    
    if proba > max_proba:
        max_proba = proba
        tweet_index = probas.index(i)

test_df['probas'] = probas
filtered_rows = test_df[test_df['probas'].apply(lambda x: x[1] == max_proba)]

tweet = filtered_rows['content'].tolist()[0]
prob = max_proba


grader.check("q2.4")


vec_from_pipe = pipe_lr.named_steps["countvectorizer"]
lr_from_pipe = pipe_lr.named_steps["logisticregression"]


top_5_words = None # Store them as a list
bottom_5_words = None # Store them as a list

words = vec_from_pipe.get_feature_names_out()
coeffs = lr_from_pipe.coef_

coeff_df = pd.DataFrame(data=coeffs[0], index=words, columns=["Coefficients"])
bottom_5_df = coeff_df.sort_values(by=["Coefficients"]).iloc[:5]
top_5_df = coeff_df.sort_values(by=["Coefficients"]).iloc[27340:]

bottom_5_words = bottom_5_df['Coefficients'].index.tolist()
top_5_words = top_5_df['Coefficients'].index.tolist()


top_5_words


bottom_5_words


grader.check("q2.5")


countvec = CountVectorizer(stop_words="english")
lr = LogisticRegression(max_iter=1000, random_state=123)


fold_score = None

X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, shuffle = False, test_size = 0.2)

countvec.fit(X_tr)

X_tr_bow = countvec.transform(X_tr)
X_val_bow = countvec.transform(X_val)

lr.fit(X_tr_bow, y_tr)

fold_score = lr.score(X_val_bow, y_val)


fold_score


grader.check("q2.6")


train_scores = []
cv_scores = []

max_features = [10, 100, 1000, 10_000, 100_000]

for mf in max_features:
    #     print(mf)    
    pipe = make_pipeline(CountVectorizer(stop_words="english", max_features=mf), 
                         LogisticRegression(max_iter=1000, random_state=123))
    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)
    train_scores.append(cv_results["train_score"].mean())
    cv_scores.append(cv_results["test_score"].mean())

plt.semilogx(max_features, train_scores, label="train")
plt.semilogx(max_features, cv_scores, label="valid")
plt.legend()
plt.xlabel("max_features")
plt.ylabel("accuracy");


pd.DataFrame({"max_features": max_features, "train": train_scores, "cv": cv_scores})


train_scores = []
cv_scores = []

C_vals = 10.0 ** np.arange(-1.5, 2, 0.5)

for C in C_vals:
    #     print(C)
    pipe = make_pipeline(CountVectorizer(stop_words="english"), 
                         LogisticRegression(max_iter=1000, C=C, random_state=123))    
    cv_results = cross_validate(pipe, X_train, y_train, return_train_score=True)

    train_scores.append(cv_results["train_score"].mean())
    cv_scores.append(cv_results["test_score"].mean())

plt.semilogx(C_vals, train_scores, label="train")
plt.semilogx(C_vals, cv_scores, label="valid")
plt.legend()
plt.xlabel("C")
plt.ylabel("accuracy");


pd.DataFrame({"C": C_vals, "train": train_scores, "cv": cv_scores})


pipe_lr = make_pipeline(CountVectorizer(stop_words="english"), LogisticRegression(max_iter=1000, random_state=123))


param_grid = {
    "countvectorizer__max_features": max_features,
    "logisticregression__C": C_vals,
}
grid_search = GridSearchCV(pipe_lr, 
                           param_grid = param_grid, 
                           n_jobs=-1, 
                           return_train_score=True
                          )
grid_search.fit(X_train, y_train)

best_max_features = grid_search.best_params_["countvectorizer__max_features"]
best_C = grid_search.best_params_["logisticregression__C"]
best_score = grid_search.best_score_


grader.check("q3.3")


grid_search.best_params_


test_score = grid_search.score(X_test, y_test)


grader.check("q3.5")


print(test_score)
print(best_score)
